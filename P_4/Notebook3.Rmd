---
title: "Notebook3"
author: "Jered Hightower, Hanniyah Hammid, Sai Gonuguntla"
date: "10/8/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset

### Import our Data Set
```{r}
df <- read.csv("churn.csv")

str(df)
```

### Data Cleanup
Make all attributes numeric
```{r}
df$country<- factor(df$country)
df$gender <- factor(df$gender)


# In Years
df$age <- as.numeric(df$age)
df$tenure <- as.numeric(df$tenure)

# Number of Products
df$products_number <- as.numeric(df$products_number)

str(df)
```

### Standardize variables
```{r}
original <- df

df <- df[,-c(1, 3, 4, 8, 9, 10, 12)]
str(df)

df <- scale(df)
```

### Determine number of clusters
```{r}
set.seed(1234)

wss <- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(df, centers=i, iter.max = 20)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")


# Find optimal cluster amount
i <- sample(1:nrow(df), nrow(df)*.1, replace = FALSE)
sample <- df[i,]

library(NbClust)
nc <- NbClust(sample, min.nc=2, max.nc=10, method="kmeans")
```

### K-Means Cluster Analysis
```{r}
set.seed(1234)
fit <- kmeans(df, 2) # 2 cluster solution

# get cluster means 
aggregate(df,by=list(cluster=fit$cluster), mean)
# append cluster assignment
df <- data.frame(df, fit$cluster)
```

### Data Analysis
```{r}
gt <- table(original$gender, fit$cluster)
gt

ct <- table(original$credit_card, fit$cluster)
ct

at <- table(original$active_member, fit$cluster)
at

cht <- table(original$churn, fit$cluster)
cht

# No clear correlation to any of our factors
```

### Hierarchial Clustering
```{r}
library(flexclust)

# Subset Data to make some sense
data <- original[,-c(1, 3, 4, 8, 9 , 10, 12)]

data <- scale(data)

# Ward Hierarchical Clustering
d <- dist(data, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward") 

plot(fit) # display dendogram

groups <- cutree(fit, k=2) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters 
rect.hclust(fit, k=2, border="red")

for(c in 2:16){
  cluster_cut <- cutree(fit, c)
  table_cut <- table(cluster_cut, original$churn)
  print(table_cut)
  
  ri <- randIndex(table_cut)
  print(paste("cut=", c, "Rand index = ", ri))
}
```

### Model Based Clustering
```{r}
data <- original[,-c(1, 3, 4, 8, 9 , 10, 12)]

library(mclust)
fit <- Mclust(data)
plot(fit) # plot results 
summary(fit) # display the best model
```

### Results of the Algorithms and Insights
Overall, the algorithms showed that this dataset was not well represented by clustering.

When doing kMeans, our graph showed no clear elbow to give any idea how many clusters should be used. There were no significant knees in the Hubert index and D index graphs either. The clusters showed no significant correlation with any of the factors I excluded.

Hierarchical clustering faced similar issues since the rand index showed that no matter the clustering amount, nothing significantly correlated with churn. It was about as good as random guessing.

The Model-Based clustering suggested 7 clustering groups an was still overall. The extremely low BIC still suggested that this wasn't a good model.

In conclusion, a different model could likely represent this dataset better.
