---
title: "Classification: Logistic Regression, kNN, Decision Trees"
author: "Haniyyah Hamid, Jered Hightower, Sai Gonuguntla"
date: "10/8/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
---

# Logistic Regression

Data set used: <https://www.kaggle.com/datasets/lodetomasi1995/income-classification?datasetId=149550&language=null>

## Importing data

```{r}
data1 <- read.csv("income_dataset.csv")
str(data1)
```

## Data cleaning

We will remove unnecessary columns. We want the education.num, hours.per.week, age, and income columns. Income will be the target.

```{r}
df <- data1[,c(1, 5, 13, 15)]
df$income <- factor(df$income)
str(df)
head(df)
```

## Handle missing values

Checking to see if there are any missing data within the data frame, which there aren't.

```{r}
sapply(df, function(x) sum(is.na(x)==TRUE))
```

## Plotting data

```{r}
par(mfrow=c(1,2))
plot(df$income, df$age, xlab="income", ylab="age", varwidth=TRUE)
plot(df$income, df$education.num, xlab="income", ylab="# of education years", varwidth=TRUE)
plot(df$income, df$hours.per.week, xlab="income", ylab="hours worked per week", varwidth=TRUE)
summary(df)
```

We see that with the predictor age, the median age for a person who makes \<=50k is about 35 and about 43 for a person who makes \>50k. With the predictor being \# of years educated (how many years spent getting an education), the median years for a person who makes \<=50k is about 9 (HS grad) and about 13 (bachelors) for a person who makes \>50k. With the predictor \# of hours worked per week, the median \# of hours is about 40 for someone who makes \<50k, and this is the same median for someone for makes \>50k. However the third quartile of the box plot for someone who makes \>50k is much larger than that of someone who makes \<=50k.

## Train and test

80/20 train and test

```{r}
set.seed(1234)
i <- sample(1:nrow(df), 0.8*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
```

## Build a logistic regression model

```{r}
glm1 <- glm(income~., data=train, family="binomial")
summary(glm1)
```

We see that the P value on all the predictors indicate that they are very good predictors for the target. Age has the least standard error (0.001298) of the 3 predictors, while the number of education years has the highest (0.007254).

## Evaluate on the test set

```{r}
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, " >50K", " <=50K")
acc <- mean(pred==test$income)
print(paste("accuracy = ", acc))
table(pred, test$income)
```

## Confusion matrix

```{r}
library(caret)
confusionMatrix(as.factor(pred), reference=test$income)
```

## Analysis of the Logistic Regression model on the dataset

We find that the accuracy is 0.7884, which implies that the logistic regression model accurate enough to predict future values of the dataset. We find that the sensitivity was 0.9388, showing that the true positive rate is quite high and accurate at predicting true results. We also find that the specificity was 0.3231, showing that the true negative rate is quite low and not as accurate at predicting false results. Therefore, based off these results logistic regression can show that age, number of education years, and number of hours worked per week are great predictors for determining if a person makes \<=50k or \>50k. The model itself is not entirely but at least decently accurate at predicting future results.

# kNN Classification

## Read in data in a new dataframe

```{r}
df2 <- data1[,c(1, 5, 13, 15)]
str(df2)
```

## Plotting

```{r}
plot(df2$age, df2$hours.per.week, pch=21, bg=c("red", "blue") [unclass(df2$income)], main="Income data")
```

## Pair scatter plots

```{r}
pairs(df2[1:3], main="Income Data", pch=21, bg=c("red", "green3")[unclass(df2$income)])
```

## Divide into train/test sets

```{r}
set.seed(1958)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.67, 0.33))
df2.train <- df2[ind==1, 1:3]
df2.test <- df2[ind==2, 1:3]
df2.trainLabels <- df2[ind==1, 4]
df2.testLabels <- df2[ind==2, 4]
```

## Classify

```{r}
library(class)
df2_pred <- knn(train=df2.train, test=df2.test, cl=df2.trainLabels, k=3)
```

## Compute accuracy

```{r}
results <- df2_pred == df2.testLabels
acc <- length(which(results==TRUE)) / length(results)
print(paste("accuracy = ", acc))
table(results, df2_pred)
```

With kNN classification we find that we get an accuracy of about 0.78. Meaning that with knn clustering, we find that we are able to classify the results fairly accurately.

# Decision Trees

## Read in data in a new dataframe

```{r}
df3 <- data1[,c(1, 5, 13, 15)]
str(df3)
```

## Using rpart

```{r}
library(rpart)
tree1 <- rpart(df3$income~., data=df3, method="class")
tree1
```

## Plotting the rpart tree

```{r}
plot(tree1, uniform=TRUE, margin=0.2)
text(tree1)
```

## Using tree() package with training data

```{r}
library(tree)
set.seed(1958)
j <- sample(1:nrow(df), 0.8*nrow(df), replace=FALSE)
train <- df3[j,]
test <- df3[-j, ]
tree2 <- tree(as.factor(income)~., data=train)
tree_pred <- predict(tree2, newdata=test, type="class")
table(tree_pred, test$income)
mean(tree_pred==test$income)
```

We find that with making a tree using training data and then evaluating it on test data, the accuracy of the decision tree was \~0.79. This value is very similar to the accuracy found when using the logistic regression model to predict the results.
